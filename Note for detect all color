import cv2
import pytesseract
import csv
import os
import re
import numpy as np
import socket
from collections import deque
import threading
import time

# Set the mode
real_time_mode = True

# Specify the path to the image file
image_path = "GrayCO2.JPEG"

OCR = True

# Initialize the camera (only used in real-time mode)
if real_time_mode:
    cap = cv2.VideoCapture(5)
    fps = 25  # Desired frame rate (fps)
    cap.set(cv2.CAP_PROP_FPS, fps)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, int(640 * 0.85))
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, int(480 * 0.85))

# Define the expected OCR pattern
expected_pattern = r'A\d{10}'
detected_text_history = deque(maxlen=5)  # Keep track of previous OCR results

# TCP/IP server settings for PLC trigger
PLC_IP = "0.0.0.0"  # The server IP address (0.0.0.0 listens to all interfaces)
PLC_PORT = 5000    # The port to listen to PLC signals

# Shared variables and locks for frames and processed results
latest_frame = None
frame_lock = threading.Lock()

latest_processed_frame = None
processed_frame_lock = threading.Lock()

# Shared variable for stable_text
stable_text_lock = threading.Lock()
stable_text = ""  # Initialized globally

# Shared variables and locks for mean_intensity and background_type
mean_intensity_lock = threading.Lock()
mean_intensity_value = 0
background_type_value = ''

# Lock for crop_coords
crop_coords_lock = threading.Lock()

# Variables for mouse drawing
drawing = False  # True if the mouse is pressed
ix, iy = -1, -1  # Initial x and y coordinates
temp_crop_coords = None  # Temporary crop coordinates during drawing

# Store the connection globally so we can send data manually
plc_connection = None
plc_address = None
plc_connected = False

# Function to handle PLC trigger and send OCR result
def handle_plc_connection(conn, addr):
    global plc_connection, plc_address, plc_connected
    plc_connection = conn
    plc_address = addr
    plc_connected = True
    print(f"Connection established with {addr}")
    try:
        while True:
            # Receive the trigger signal from PLC
            data = conn.recv(1024)
            if data:
                print(f"Received trigger from PLC: {data.decode()}")
                # Simulate OCR processing delay
                time.sleep(2)  # Reduced delay for OCR result processing
                # Send back the OCR result
                with stable_text_lock:
                    response = stable_text.encode()  # Simulate sending the latest OCR result
                conn.sendall(response)
                print(f"Sent OCR result: {response.decode()}")
            else:
                print(f"Connection closed by PLC: {addr}")
                break
    except ConnectionResetError:
        print(f"Connection lost with PLC: {addr}")
    finally:
        plc_connected = False
        plc_connection = None
        plc_address = None

# TCP Server to listen for PLC triggers
def plc_server():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket:
        server_socket.bind((PLC_IP, PLC_PORT))
        server_socket.listen(1)
        print(f"Tinker Board server listening on {PLC_IP}:{PLC_PORT}...")
        while True:
            conn, addr = server_socket.accept()
            threading.Thread(target=handle_plc_connection, args=(conn, addr)).start()

# Function to manually send OCR result when "c" is pressed
def manual_ocr_sender():
    global plc_connection, plc_connected

    print("Press 'c' to manually send the OCR result to the connected PLC.")

    while True:
        if cv2.waitKey(1) & 0xFF == ord('c'):
            if plc_connected and plc_connection:
                with stable_text_lock:
                    response = stable_text.encode()  # Simulate sending the latest OCR result
                try:
                    plc_connection.sendall(response)
                    print(f"Manually sent OCR result: {response.decode()} to PLC: {plc_address}")
                except BrokenPipeError:
                    print("Error: Connection to PLC lost. Unable to send OCR result.")
            else:
                print("No PLC connected. Unable to send OCR result.")

# OCR processing in a separate thread
def process_ocr():
    global detected_text_history, stable_text, latest_frame, latest_processed_frame
    global mean_intensity_value, background_type_value
    while True:
        # Get the latest frame
        with frame_lock:
            if latest_frame is None:
                continue
            frame = latest_frame.copy()

        # Process frame for OCR
        with crop_coords_lock:
            x1, y1, x2, y2 = crop_coords
        cropped = frame[y1:y2, x1:x2]

        # Detect background type
        mean_intensity, background_type = detect_background_type(cropped)

        # Store mean_intensity and background_type in shared variables
        with mean_intensity_lock:
            mean_intensity_value = mean_intensity
            background_type_value = background_type

        # Get the appropriate parameters based on background type
        if background_type == 'Light':
            block_size = cv2.getTrackbarPos('Block Size Light', 'OCR Combined Video')
            c_value = cv2.getTrackbarPos('C Light', 'OCR Combined Video')
            threshold_type = cv2.THRESH_BINARY_INV
        else:  # 'Dark'
            block_size = cv2.getTrackbarPos('Block Size Dark', 'OCR Combined Video')
            c_value = -cv2.getTrackbarPos('C Dark', 'OCR Combined Video')
            threshold_type = cv2.THRESH_BINARY

        # Ensure block_size is odd and greater than or equal to 3
        if block_size % 2 == 0:
            block_size += 1
        if block_size < 3:
            block_size = 3

        # Resize the cropped image
        resize_scale = cv2.getTrackbarPos('Resize Scale (%)', 'OCR Combined Video')
        scale_factor = max(resize_scale / 100.0, 0.1)
        resized_cropped = cv2.resize(cropped, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LINEAR)

        gray = cv2.cvtColor(resized_cropped, cv2.COLOR_BGR2GRAY)
        clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(2, 2))
        equalized = clahe.apply(gray)
        denoised = remove_noise(equalized, cv2.getTrackbarPos('Blur Kernel Size', 'OCR Combined Video'))

        # Sharpen the image
        sharpened = sharpen_image(denoised)

        # Adaptive thresholding
        thresh = cv2.adaptiveThreshold(
            sharpened, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            threshold_type, block_size, c_value
        )

        erosion_kernel_size = cv2.getTrackbarPos('Erosion Kernel Size', 'OCR Combined Video')
        dilation_kernel_size = cv2.getTrackbarPos('Dilation Kernel Size', 'OCR Combined Video')
        morphed = apply_morphology(thresh, erosion_kernel_size, dilation_kernel_size)

        # Contour filtering to eliminate small noise
        min_contour_area = cv2.getTrackbarPos('Min Contour Area', 'OCR Combined Video')
        contours, hierarchy = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        mask = np.zeros_like(morphed)
        for contour in contours:
            area = cv2.contourArea(contour)
            if area >= min_contour_area:
                cv2.drawContours(mask, [contour], -1, 255, thickness=cv2.FILLED)
        morphed = cv2.bitwise_and(morphed, mask)

        # Perform OCR
        custom_config = r'--oem 3 --psm 7 -c tessedit_char_whitelist=0123456789A'
        ocr_result = pytesseract.image_to_string(morphed, config=custom_config)
        detected_text = ''.join(filter(str.isalnum, ocr_result))
        detected_text_history.append(detected_text)

        # Lock and update the stable text
        with stable_text_lock:
            stable_text = get_stabilized_text(detected_text_history)

# Main loop
def main():
    global stable_text, latest_frame, latest_processed_frame
    global mean_intensity_value, background_type_value

    plc_thread = threading.Thread(target=plc_server, daemon=True)
    plc_thread.start()

    ocr_thread = threading.Thread(target=process_ocr, daemon=True)
    ocr_thread.start()

    manual_sender_thread = threading.Thread(target=manual_ocr_sender, daemon=True)
    manual_sender_thread.start()

    try:
        if real_time_mode:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # Update the latest frame
                with frame_lock:
                    latest_frame = frame.copy()

                # Lock and display stable_text
                with stable_text_lock:
                    cv2.putText(frame, f"Detected: {stable_text}", (10, 30),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    cv2.putText(frame, f"Match: {'OK' if re.match(expected_pattern, stable_text) else 'Not match'}",
                                (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                # Lock and get mean_intensity and background_type
                with mean_intensity_lock:
                    mean_intensity_display = mean_intensity_value
                    background_type_display = background_type_value

                # Display mean_intensity and background_type on the frame
                cv2.putText(frame, f"Mean Intensity: {mean_intensity_display:.2f}", (10, 90),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(frame, f"Background: {background_type_display}", (10, 120),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                # Draw the crop rectangle
                if drawing and temp_crop_coords is not None:
                    x1_t, y1_t, x2_t, y2_t = temp_crop_coords
                    cv2.rectangle(frame, (x1_t, y1_t), (x2_t, y2_t), (0, 0, 255), 1)
                else:
                    with crop_coords_lock:
                        x1, y1, x2, y2 = crop_coords
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)

                # Get the latest processed frame
                with processed_frame_lock:
                    if latest_processed_frame is not None:
                        last_processed_frame = latest_processed_frame.copy()
                    else:
                        last_processed_frame = None

                if last_processed_frame is not None:
                    combined = np.hstack((frame, last_processed_frame))
                else:
                    blank_image = np.zeros_like(frame)
                    combined = np.hstack((frame, blank_image))

                cv2.imshow('OCR Combined Video', combined)

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
    finally:
        if real_time_mode and cap.isOpened():
            cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
